{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=review[review[\"Score\"]!=3]\n",
    "def partition(x):\n",
    "    if x > 3:\n",
    "        return 1\n",
    "    return 0\n",
    "review['Score']=review.Score.map(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = review.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "review = review[review['HelpfulnessNumerator'] <= review['HelpfulnessDenominator']]\n",
    "review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = review.loc[40000:90000,['Summary','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                          Summary  Score\n",
       "40000                                Oh yeah!!!!      1\n",
       "40001  Great, but not for an air popper for sure      1\n",
       "40002                          Kid Safe Popcorn!      1\n",
       "40003                                      Yummy      1\n",
       "40004                                Not so good      0\n",
       "...                                          ...    ...\n",
       "89996                     The dogs rate it *****      1\n",
       "89997                                    Wedding      1\n",
       "89998                                  Delicious      1\n",
       "89999              Liked these better as a kid..      1\n",
       "90000                               Disappointed      0\n",
       "\n",
       "[39207 rows x 2 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Removing Stop Word--------\n",
      "-------Replace Abbreviations--------\n",
      "-------Stemming--------\n",
      "-------Lemmazation--------\n",
      "-------Capitalization--------\n"
     ]
    }
   ],
   "source": [
    "import nltk,sys \n",
    "import nltk.data\n",
    "from nltk.stem.porter import *\n",
    "pd.options.mode.chained_assignment = None\n",
    "#from spellchecker import SpellChecker\n",
    "#https://emojipedia.org/people\n",
    "from textblob import TextBlob\n",
    "# step 1 stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "#print(['Summary'])\n",
    "\n",
    "print('-------Removing Stop Word--------')\n",
    "df['Step1_SentimentText'] = df['Summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# step 2 replace special char and replace abbreviations\n",
    "import csv,re\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "\n",
    "        # File Access mode [Read Mode]\n",
    "        with open(fileName, \"r\") as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9]+', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    return ' '.join(user_string)\n",
    "print('-------Replace Abbreviations--------')\n",
    "df['Step2_SentimentText'] = df['Step1_SentimentText'].apply(lambda x:  translator(x)  ) \n",
    "\n",
    "# step 3 stemming \n",
    "ps = PorterStemmer()\n",
    "print('-------Stemming--------')\n",
    "df['Step3_SentimentText'] = df['Step2_SentimentText'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "\n",
    "# step 4 Lemmazation\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "print('-------Lemmazation--------')\n",
    "df['Step4_SentimentText'] = df['Step2_SentimentText'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "# step 6 Capitalization\n",
    "print('-------Capitalization--------')\n",
    "df['Step6_SentimentText'] = df['Step2_SentimentText'].apply(  lambda x: ' '.join( [ word.upper() for word in x.split() ] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "      <th>Step1_SentimentText</th>\n",
       "      <th>Step2_SentimentText</th>\n",
       "      <th>Step3_SentimentText</th>\n",
       "      <th>Step4_SentimentText</th>\n",
       "      <th>Step6_SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>Oh yeah!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh yeah!!!!</td>\n",
       "      <td>Oh yeah!!!!</td>\n",
       "      <td>Oh yeah!!!!</td>\n",
       "      <td>Oh yeah!!!!</td>\n",
       "      <td>OH YEAH!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40001</td>\n",
       "      <td>Great, but not for an air popper for sure</td>\n",
       "      <td>1</td>\n",
       "      <td>Great, air popper sure</td>\n",
       "      <td>Great, air popper sure</td>\n",
       "      <td>great, air popper sure</td>\n",
       "      <td>Great, air popper sure</td>\n",
       "      <td>GREAT, AIR POPPER SURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40002</td>\n",
       "      <td>Kid Safe Popcorn!</td>\n",
       "      <td>1</td>\n",
       "      <td>Kid Safe Popcorn!</td>\n",
       "      <td>Kid Safe Popcorn!</td>\n",
       "      <td>kid safe popcorn!</td>\n",
       "      <td>Kid Safe Popcorn!</td>\n",
       "      <td>KID SAFE POPCORN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40003</td>\n",
       "      <td>Yummy</td>\n",
       "      <td>1</td>\n",
       "      <td>Yummy</td>\n",
       "      <td>Yummy</td>\n",
       "      <td>yummi</td>\n",
       "      <td>Yummy</td>\n",
       "      <td>YUMMY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40004</td>\n",
       "      <td>Not so good</td>\n",
       "      <td>0</td>\n",
       "      <td>Not good</td>\n",
       "      <td>Not good</td>\n",
       "      <td>not good</td>\n",
       "      <td>Not good</td>\n",
       "      <td>NOT GOOD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Summary  Score  \\\n",
       "40000                                Oh yeah!!!!      1   \n",
       "40001  Great, but not for an air popper for sure      1   \n",
       "40002                          Kid Safe Popcorn!      1   \n",
       "40003                                      Yummy      1   \n",
       "40004                                Not so good      0   \n",
       "\n",
       "          Step1_SentimentText     Step2_SentimentText     Step3_SentimentText  \\\n",
       "40000             Oh yeah!!!!             Oh yeah!!!!             Oh yeah!!!!   \n",
       "40001  Great, air popper sure  Great, air popper sure  great, air popper sure   \n",
       "40002       Kid Safe Popcorn!       Kid Safe Popcorn!       kid safe popcorn!   \n",
       "40003                   Yummy                   Yummy                   yummi   \n",
       "40004                Not good                Not good                not good   \n",
       "\n",
       "          Step4_SentimentText     Step6_SentimentText  \n",
       "40000             Oh yeah!!!!             OH YEAH!!!!  \n",
       "40001  Great, air popper sure  GREAT, AIR POPPER SURE  \n",
       "40002       Kid Safe Popcorn!       KID SAFE POPCORN!  \n",
       "40003                   Yummy                   YUMMY  \n",
       "40004                Not good                NOT GOOD  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Step6_SentimentText']\n",
    "Y = df['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(xtrain)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31365x8907 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 99822 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vect = vect.transform(xtrain)\n",
    "x_train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer().fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range = (1,2)).fit(xtrain)\n",
    "x_train_vect = vect.transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_vect,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7335454066030865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "predictions = model.predict(vect.transform(xtest))\n",
    "print('AUC: ',roc_auc_score(ytest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(x_train_vect,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.775934142991537\n"
     ]
    }
   ],
   "source": [
    "predictions1 = model1.predict(vect.transform(xtest))\n",
    "print('AUC: ',roc_auc_score(ytest,predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(model1.predict(vect.transform(['i m gr8']))) #great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(model1.predict(vect.transform(['worst product ever'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(model1.predict(vect.transform([': )'])))  #posit-ive --1 or neg -- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verry baad\n",
      "did you mean sent very bad if yes then 1 else 0\n",
      "1\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sent = input()\n",
    "sent_textblob = TextBlob(sent)\n",
    "sent_correct = sent_textblob.correct()\n",
    "sent_correct = translator(sent_correct)\n",
    "print(\"did you mean sent {} if yes then 1 else 0\".format(sent_correct))\n",
    "n = int(input())\n",
    "if n:\n",
    "    print(model.predict(vect.transform([sent_correct])))\n",
    "else:\n",
    "    print(model.predict(vect.transform([sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
